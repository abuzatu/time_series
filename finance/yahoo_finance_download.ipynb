{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Reading and analysing data from Yahoo Finance\n",
    "\n",
    "Taking the functions from utils.py, to make the code more modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging level: NOTSET, DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER_NAME_INFO = \"/Users/abuzatu/Work/data/finance/stocks/info\"\n",
    "INPUT_FILE_NAME = \"/Users/abuzatu/Work/data/finance/stocks/tickers1.txt\"\n",
    "INPUT_FOLDER_NAME = \"/Users/abuzatu/Work/data/finance/stocks/processed_data\"\n",
    "OUTPUT_FOLDER_NAME = \"/Users/abuzatu/Work/data/finance/stocks/processed_data/210111\"\n",
    "\n",
    "# creates output folder\n",
    "Path(OUTPUT_FOLDER_NAME).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIOD = None\n",
    "ADD_OUTSIDE_TRADING_HOURS = True\n",
    "ADD_DIVIDENDS_AND_STOCK_SPLITS = True\n",
    "AUTO_ADJUST = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    LIST_DATE = [\n",
    "        (pd.to_datetime(\"2021-01-04\"), pd.to_datetime(\"2021-01-11\"), \"1m\")\n",
    "    ]\n",
    "else:\n",
    "    LIST_DATE = None\n",
    "LIST_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stock_ticker_folder = get_list_stock_ticker_in_folder(INPUT_FOLDER_NAME)\n",
    "list_stock_ticker_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stock_ticker_file = get_list_stock_ticker_from_file(INPUT_FILE_NAME)\n",
    "list_stock_ticker_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_all_stock_stickers(INPUT_FOLDER_NAME_INFO, INPUT_FOLDER_NAME, INPUT_FILE_NAME, OUTPUT_FOLDER_NAME, ADD_OUTSIDE_TRADING_HOURS, ADD_DIVIDENDS_AND_STOCK_SPLITS, AUTO_ADJUST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"/Users/abuzatu/Work/data/finance/stocks/processed_data/210102/df_20-12-04_20-12-11_1m_TSLA.pickle\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle(\"/Users/abuzatu/Work/data/finance/stocks/processed_data/210102/df_20-12-11_20-12-18_1m_TSLA.pickle\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle(\"/Users/abuzatu/Work/data/finance/stocks/processed_data/210109/df_2020-12-12_2020-12-19_1m_TSLA.pickle\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(f\"{INPUT_FOLDER_NAME_INFO}/info.pickle\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([], columns = [\"stock_ticker\", \"date_first\"])\n",
    "df.set_index(\"stock_ticker\", inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"ENG\"][\"date_first\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"{INPUT_FOLDER_NAME_INFO}/info.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(f\"{INPUT_FOLDER_NAME_INFO}/info.pickle\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ENG2\" in df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new element\n",
    "df.loc[\"ENG\"] = \"d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"BLA\"] = \"b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df.index)]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)\n",
    "df.append({\"date_first\" : \"2020-08-01\"}, index = \"NIO\", ignore_index=True, sort=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file and save to a file\n",
    "str_today = str(pd.Timestamp.today().date())\n",
    "logging.info(f\"Today is {str_today}\")\n",
    "for stock_ticker in list_stock_ticker_file:\n",
    "    \n",
    "    if False:\n",
    "        # do only for one ticker\n",
    "        if stock_ticker != \"ZOM\":\n",
    "            continue\n",
    "    already_have = stock_ticker in list_stock_ticker\n",
    "    logging.info(f\"stock_ticker={stock_ticker}, already_have={already_have}\")\n",
    "    \n",
    "    continue\n",
    "    \n",
    "    # create list of dates\n",
    "    if LIST_DATE is None:\n",
    "        # find automatically the range that we want\n",
    "        # we collect the data using the period max, then find the first date\n",
    "        # than depending on that date build the LIST_DATE\n",
    "        df = read_data(stock_ticker,\n",
    "                   \"max\",\n",
    "                   None,\n",
    "                   None,\n",
    "                   \"1d\",\n",
    "                   ADD_OUTSIDE_TRADING_HOURS,\n",
    "                   ADD_DIVIDENDS_AND_STOCK_SPLITS,\n",
    "                   AUTO_ADJUST)\n",
    "    \n",
    "        # calculate list_date\n",
    "        str_date_start = str(df.index[0].date())\n",
    "        str_date_end = str_today\n",
    "        logging.info(f\"str_date_start={str_date_start}, str_date_end={str_date_end}\")\n",
    "        list_date = get_list_date(str_date_start, str_date_end)\n",
    "    else:\n",
    "        list_date = LIST_DATE\n",
    "    for date in list_date:\n",
    "        logging.info(f\"{date}\")\n",
    "        \n",
    "    do_store_all_period_to_file = False\n",
    "    do_store_only_some_period_to_file = False\n",
    "    df = get_df_from_list_date(stock_ticker, list_date, OUTPUT_FOLDER_NAME, PERIOD, ADD_OUTSIDE_TRADING_HOURS,  ADD_DIVIDENDS_AND_STOCK_SPLITS, AUTO_ADJUST, do_store_all_period_to_file, do_store_only_some_period_to_file)\n",
    "    \n",
    "    do_combine_with_previous_file = True   \n",
    "    if do_combine_with_previous_file:\n",
    "        logging.debug(\"Start do_combine_with_previous_file.\")\n",
    "        # retrieve the previous file\n",
    "        date_start_all = \"2019-01-01\"\n",
    "        date_end_all = \"2021-01-01\"\n",
    "        suffix = \"al\"\n",
    "        input_file_name_data = f\"{OUTPUT_FOLDER_NAME}/df_{date_start_all}_{date_end_all}_{suffix}_{stock_ticker}.pickle\"\n",
    "        df__ = pd.read_pickle(input_file_name_data)\n",
    "        logging.debug(f\"Previous file {input_file_name_data} has {len(df__)} entries.\")\n",
    "    \n",
    "        logging.debug(\"Start concatenating the old and new file\")\n",
    "        df_ = pd.concat([df__, df], axis = 0)\n",
    "        logging.debug(\"Sort by index in chronolical order\")\n",
    "        df_.sort_index(inplace = True)\n",
    "        logging.debug(\"Remove potential rare wrong values for after hours that are just to huge.\")\n",
    "        df_ = df_[df_.Close < 1e6] # eliminate sometimes very high wrong values\n",
    "    \n",
    "        # write to file\n",
    "        date_end_all = str(LIST_DATE[-1][1].date())\n",
    "        output_file_name_data = f\"{OUTPUT_FOLDER_NAME}/df_{date_start_all}_{date_end_all}_{suffix}_{stock_ticker}.pickle\"\n",
    "        df_.to_pickle(output_file_name_data)\n",
    "        logging.debug(f\"Written final df_ with {len(df_)} elements to file {output_file_name_data}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_ticker =  \"ENG\"\n",
    "df1 = read_data(stock_ticker,\n",
    "                   \"max\",\n",
    "                   None,\n",
    "                   None,\n",
    "                   \"1d\",\n",
    "                   ADD_OUTSIDE_TRADING_HOURS,\n",
    "                   ADD_DIVIDENDS_AND_STOCK_SPLITS,\n",
    "                   AUTO_ADJUST)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[df1.index > pd.to_datetime(\"2020-06-01\").tz_localize(LOCALIZE_US_STOCK_MARKET)]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df2.Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = read_data(stock_ticker,\n",
    "                   None,\n",
    "                   pd.to_datetime(\"2020-11-11\").tz_localize(LOCALIZE_US_STOCK_MARKET),\n",
    "                   pd.to_datetime(\"2020-12-11\").tz_localize(LOCALIZE_US_STOCK_MARKET),\n",
    "                   \"5m\",\n",
    "                   ADD_OUTSIDE_TRADING_HOURS,\n",
    "                   ADD_DIVIDENDS_AND_STOCK_SPLITS,\n",
    "                   AUTO_ADJUST)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_pickle(\"/Users/abuzatu/Work/data/finance/stocks/processed_data/210109/df_2020-11-11_2020-12-11_5m_ENG.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df22 = df2.copy()\n",
    "df22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = read_data(stock_ticker,\n",
    "                   None,\n",
    "                   pd.to_datetime(\"2020-11-11\"),#.tz_localize(\"Europe/Berlin\"),\n",
    "                   pd.to_datetime(\"2020-12-11\"),#.tz_localize(\"Europe/Berlin\"),\n",
    "                   \"5m\",\n",
    "                   ADD_OUTSIDE_TRADING_HOURS,\n",
    "                   ADD_DIVIDENDS_AND_STOCK_SPLITS,\n",
    "                   AUTO_ADJUST)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df32 = read_data(stock_ticker,\n",
    "                   None,\n",
    "                   pd.to_datetime(\"2020-11-11\"),#.tz_localize(\"Europe/Berlin\"),\n",
    "                   pd.to_datetime(\"2020-12-11\"),#.tz_localize(\"Europe/Berlin\"),\n",
    "                   \"5m\",\n",
    "                   ADD_OUTSIDE_TRADING_HOURS,\n",
    "                   ADD_DIVIDENDS_AND_STOCK_SPLITS,\n",
    "                   AUTO_ADJUST)\n",
    "df32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df32[df32.index > pd.to_datetime(\"2020-11-11\").tz_localize(LOCALIZE_US_STOCK_MARKET)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_pickle(f\"{OUTPUT_FOLDER_NAME}/210109/df_2020-11-11_2020-12-11_2m_ENG.pickle\")\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(df.index[0].date())[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = df.index[0]\n",
    "date_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_end = pd.Timestamp.today()\n",
    "date_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = pd.to_datetime(pd.Timestamp.today().date())\n",
    "date_today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_end = pd.to_datetime(\"2021-01-01\")\n",
    "date_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = pd.to_datetime(pd.Timestamp.today())\n",
    "start = end - pd.Timedelta(60, \"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = pd.to_datetime(pd.Timestamp.today().date()).tz_localize(LOCALIZE_US_STOCK_MARKET)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start_1d = pd.to_datetime(\"2003-07-15\")\n",
    "date_today = pd.Timestamp.today().normalize()\n",
    "date_start_1h = date_today - pd.Timedelta(730 - 1, \"d\")\n",
    "date_start_2m = date_today - pd.Timedelta(60 - 1, \"d\")\n",
    "date_start_1m = date_today - pd.Timedelta(30 - 1 , \"d\")\n",
    "date_end = pd.to_datetime(\"2020-12-27\")\n",
    "date_end = date_today\n",
    "\n",
    "print(f\"date_start_1d={date_start_1d}\")\n",
    "print(f\"date_start_1h={date_start_1h}\")\n",
    "print(f\"date_start_2m={date_start_2m}\")\n",
    "print(f\"date_start_1m={date_start_1m}\")\n",
    "print(f\"date_today   ={date_today}\")\n",
    "print(f\"date_end     ={date_end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_date():\n",
    "    list_date = []\n",
    "    if date_end <= date_start_1d:\n",
    "        print(\"A\")\n",
    "        raise RuntimeError(f\"date_end={date_end} <= date_start_1d={date_start_1d}\")\n",
    "    elif date_end <= date_start_1h:\n",
    "        print(\"B\")\n",
    "        list_date.append((date_start_1d, date_end, \"1d\"))\n",
    "    elif date_end <= date_start_2m:\n",
    "        print(\"C\")\n",
    "        list_date.append((date_start_1d, date_start_1h, \"1d\"))\n",
    "        list_date.append((date_start_1h, date_end, \"1h\"))\n",
    "    elif date_end <= date_start_1m:\n",
    "        print(\"D\")\n",
    "        list_date.append((date_start_1d, date_start_1h, \"1d\"))\n",
    "        list_date.append((date_start_1h, date_start_2m, \"1h\"))\n",
    "        list_date.append((date_start_2m, date_end, \"2m\"))\n",
    "    elif date_end <= date_today:\n",
    "        print(\"E\")\n",
    "        #list_date.append((date_start_1d, date_start_1h, \"1d\"))\n",
    "        #list_date.append((date_start_1h, date_start_2m, \"1h\"))\n",
    "        #list_date.append((date_start_2m, date_start_1m, \"2m\"))\n",
    "        # now comes an extra constraint that in this last 30 days interval\n",
    "        # we can query for 1 minute interval in just 7 days long intervals\n",
    "        # so we need to query several times (maximum 4 times)\n",
    "        # first let's evaluate the boundaries for these\n",
    "        date_start_1m_0 = date_start_1m + pd.Timedelta(0 * 7, \"d\")\n",
    "        date_start_1m_1 = date_start_1m + pd.Timedelta(1 * 7, \"d\")\n",
    "        date_start_1m_2 = date_start_1m + pd.Timedelta(2 * 7, \"d\")\n",
    "        date_start_1m_3 = date_start_1m + pd.Timedelta(3 * 7, \"d\")\n",
    "        date_start_1m_4 = date_start_1m + pd.Timedelta(4 * 7, \"d\")\n",
    "        print(f\"date_start_1m_0={date_start_1m_0}\")\n",
    "        print(f\"date_start_1m_1={date_start_1m_1}\")\n",
    "        print(f\"date_start_1m_2={date_start_1m_2}\")\n",
    "        print(f\"date_start_1m_3={date_start_1m_3}\")\n",
    "        print(f\"date_start_1m_4={date_start_1m_4}\")\n",
    "        if date_end <= date_start_1m_1:\n",
    "            list_date.append((date_start_1m_0, date_end, \"1m\"))\n",
    "        elif date_end <= date_start_1m_2:\n",
    "            list_date.append((date_start_1m_0, date_start_1m_1, \"1m\"))\n",
    "            list_date.append((date_start_1m_1, date_end, \"1m\"))\n",
    "        elif date_end <= date_start_1m_3:\n",
    "            list_date.append((date_start_1m_0, date_start_1m_1, \"1m\"))\n",
    "            list_date.append((date_start_1m_1, date_start_1m_2, \"1m\"))\n",
    "            list_date.append((date_start_1m_2, date_end, \"1m\"))\n",
    "        elif date_end <= date_start_1m_4:\n",
    "            list_date.append((date_start_1m_0, date_start_1m_1, \"1m\"))\n",
    "            list_date.append((date_start_1m_1, date_start_1m_2, \"1m\"))\n",
    "            list_date.append((date_start_1m_2, date_start_1m_3, \"1m\"))\n",
    "            list_date.append((date_start_1m_3, date_end, \"1m\"))\n",
    "        else:\n",
    "            #list_date.append((date_start_1m_0, date_start_1m_1, \"1m\"))\n",
    "            #list_date.append((date_start_1m_1, date_start_1m_2, \"1m\"))\n",
    "            list_date.append((date_start_1m_2, date_start_1m_3, \"1m\"))\n",
    "            list_date.append((date_start_1m_3, date_start_1m_4, \"1m\"))\n",
    "            list_date.append((date_start_1m_4, date_end, \"1m\"))\n",
    "        \n",
    "    else:\n",
    "        raise RuntimeError(f\"date_end={date_end} > date_today={date_today}\")\n",
    "    #list_date.append((, , \"1d\"))\n",
    "    return list_date\n",
    "list_date = get_list_date()\n",
    "list_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_2(stock_ticker, list_date, output_folder_name):\n",
    "    list_df = []\n",
    "    for s, e, interval in list_date:\n",
    "        #string_date_start = f\"20{s} 00:00:00\"\n",
    "        #string_date_end   = f\"20{e} 00:00:00\"\n",
    "        #date_start = pd.to_datetime(string_date_start).tz_localize(LOCALIZE_US_STOCK_MARKET)\n",
    "        #date_end = pd.to_datetime(string_date_end).tz_localize(LOCALIZE_US_STOCK_MARKET)\n",
    "        date_start = s.tz_localize(LOCALIZE_US_STOCK_MARKET)\n",
    "        date_end = e.tz_localize(LOCALIZE_US_STOCK_MARKET)\n",
    "        logger.info(f\"{stock_ticker} from {date_start} to {date_end} with interval {interval}\")\n",
    "\n",
    "        # read the data\n",
    "        df = read_data(stock_ticker,\n",
    "                   PERIOD,\n",
    "                   date_start,\n",
    "                   date_end,\n",
    "                   interval,\n",
    "                   ADD_OUTSIDE_TRADING_HOURS,\n",
    "                   ADD_DIVIDENDS_AND_STOCK_SPLITS,\n",
    "                   AUTO_ADJUST)\n",
    "        \n",
    "        #\n",
    "        logger.info(f\"len = {len(df)}\")\n",
    "        if len(df) > 0:\n",
    "            if stock_ticker == \"AMRH\":\n",
    "                if interval.endswith(\"h\") or interval.endswith(\"m\"):\n",
    "                    # ajust by the stock split of 4 stocks -> 1 stock\n",
    "                    apply_split(df, 4, 1)\n",
    "            # add to list\n",
    "            list_df.append(df)\n",
    "            # save for future\n",
    "            ss = str(s.date())\n",
    "            se = str(e.date())\n",
    "            output_file_name = get_output_file_name(output_folder_name, ss, se, interval, stock_ticker)\n",
    "            df.to_pickle(output_file_name )\n",
    "                  \n",
    "    # print(list_df[-1])\n",
    "    return pd.concat(list_df, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"{stock_ticker}\")\n",
    "df = get_df_2(stock_ticker, list_date, OUTPUT_FOLDER_NAME)\n",
    "date_start_all = str(list_date[0][0].date())\n",
    "date_end_all = str(list_date[-1][1].date())\n",
    "output_file_name = get_output_file_name(OUTPUT_FOLDER_NAME, date_start_all, date_end_all, \"al\", stock_ticker)\n",
    "df.to_pickle(output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = yf.Ticker(stock_ticker)\n",
    "#end = pd.to_datetime(pd.Timestamp.today().date()).tz_localize(LOCALIZE_US_STOCK_MARKET)\n",
    "#start = end - pd.Timedelta(59, \"d\")\n",
    "#end = pd.to_datetime(pd.Timestamp.today())\n",
    "# end = pd.to_datetime(pd.Timestamp.today().date())#.tz_localize(LOCALIZE_US_STOCK_MARKET)\n",
    "#end = pd.to_datetime(pd.Timestamp.today().date()).tz_localize(\"Europe/Berlin\")\n",
    "#start = end - pd.Timedelta(7, \"d\")\n",
    "start = pd.to_datetime(\"2020-12-10 16:10:00\")\n",
    "end = pd.to_datetime(\"2020-12-10 16:15:00\")\n",
    "print(start)\n",
    "print(end)\n",
    "df = ticker.history(\n",
    "        period = None,\n",
    "        # start = pd.to_datetime(\"2021-01-01\").tz_localize(LOCALIZE_US_STOCK_MARKET),\n",
    "        # end = pd.to_datetime(\"2021-01-09\").tz_localize(LOCALIZE_US_STOCK_MARKET),\n",
    "        # today\n",
    "        start = start,\n",
    "        end = end,\n",
    "        interval = \"1m\",\n",
    "        prepost = True,\n",
    "        actions = True,\n",
    "        auto_adjust = True,\n",
    "        )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=1604962800 and endTime=1610146800."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1604962800, 1610146800]).astype('datetime64[s]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    datetime_end = df.index[0].tz_localize(None)\n",
    "    date_short = str(datetime_end.tz_localize(None).date())[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done one for some dates\n",
    "if False:\n",
    "    df = read_data(\"LAZR\",\n",
    "                   None,\n",
    "                   pd.to_datetime(\"2019-03-25\").tz_localize(LOCALIZE_US_STOCK_MARKET),\n",
    "                   pd.to_datetime(\"2020-11-04\").tz_localize(LOCALIZE_US_STOCK_MARKET),\n",
    "                   \"1h\",\n",
    "                   ADD_OUTSIDE_TRADING_HOURS,\n",
    "                   ADD_DIVIDENDS_AND_STOCK_SPLITS,\n",
    "                   AUTO_ADJUST)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done one for the entire period\n",
    "if True:\n",
    "    df = read_data(stock_ticker,\n",
    "                   \"max\",\n",
    "                   None,\n",
    "                   None,\n",
    "                   \"1d\",\n",
    "                   ADD_OUTSIDE_TRADING_HOURS,\n",
    "                   ADD_DIVIDENDS_AND_STOCK_SPLITS,\n",
    "                   AUTO_ADJUST)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of concatenate two that changed name\n",
    "if False:\n",
    "    interval = \"al\"\n",
    "    output_file_name = get_output_file_name(OUTPUT_FOLDER_NAME, \"19-01-01\", \"20-12-31\", \"al\", \"AMRH\")\n",
    "    df1 = pd.read_pickle(output_file_name)\n",
    "    df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    interval = \"al\"\n",
    "    output_file_name = get_output_file_name(OUTPUT_FOLDER_NAME, \"20-12-31\", \"21-01-01\", \"al\", \"ENVB\")\n",
    "    df2 = pd.read_pickle(output_file_name)\n",
    "    df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df = pd.concat([df1, df2], axis = 0)\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    output_file_name = get_output_file_name(OUTPUT_FOLDER_NAME, \"19-01-01\", \"21-01-01\", \"al\", \"ENVB\")\n",
    "    df.to_pickle(output_file_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes buggy data with very large value in after-market, remove it\n",
    "df = df[df.Close < 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the pre-market data\n",
    "get_df_pre_market(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the after-market data\n",
    "get_df_after_market(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the during-market data\n",
    "df2 = get_df_during_market(df)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the in-market data, add a fictious interval one minute before that ends on the open value\n",
    "# so that we can plot the open value as well\n",
    "df3 = add_interval_with_open(df2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_plot = plot_interactive(df)\n",
    "final_plot.opts(xaxis = \"bottom\", title = f\"Stock price of {stock_ticker}\", show_legend = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_plot_volume = plot_interactive_volume(df)\n",
    "xaxis = None #Â \"bottom\"\n",
    "final_plot_volume.opts(xaxis = xaxis, yaxis = None, title = f\"Stock volume of {stock_ticker}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
